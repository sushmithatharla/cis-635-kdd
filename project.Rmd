---
title: "Comparision of Multiple Linear Regression model and Random Forest althorithm for predicting Walmart Weekly Sales"
author: "Sushmitha Tharla & Shravya Rani Damarapelli"
date: '2022-12-09'
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Introduction

In the running decades trade market has gained immense attention due to economical growth  around the World. There have been several retail companies competing in the market for enhancing their sale, but there are several controlling  factors such as economic condition, holidays,weather of the area where that retail shop belongs etc. Thus for a retail company it is very important to study the areal factors and predict expected business for a better profit and service, and developing a suitable model including all the required parameter in most convenient and easy way to accomplish. Here in this project we will compare performance of multiple linear regression(MLR) approach and Random forest(RF) approach for predicting our desired outcome, Walmart weekly sale using predictors like Holiday flag, Fuel-price, prevailing consumer price index, prevailing unemployment rate of the area and temperature based on available historical data(2010-2013). Multiple linear regression is a simple way to build a relationship between dependent and independent variables. Random forest is a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of overcoming over-fitting problem of individual decision tree. 



### Related Works

Enormous number of studies have been conducted till date for predicting Walmart sales data depending on historical data available along with studies based on comparison of different predictive models. Prediction of  next 39 weeks Walmart sales was conducted by Harsoor & Patil, 2015 using Holt’s winter algorithm, in their study they used same data used in this study. Recently a study “Walmart’s Sales Data Analysis - A Big Data Analytics Perspective,” 2017 inspected about which factors affect the sales of Walmart the most. Michael Crown (Crown, 2016) analyzed the same data set to forecast weekly sales of a year using ARIMA model and they evaluated root-mean-square error (RMSE) for model performance measurement.



Rather than only being used in trading or business related fields, Machine learning modeling and related statistical analysis now are expanding to different fields like Medical science, market analysis, weather and climate predictions etcetera. Noi et al.,2017 compared different models like Multiple Linear Regression(MLR), cubist regression and Random Forest algorithms for estimating surface air temperature. They concluded Random Forest algorithm with better $R^{2}$ value and lower RMSE, indicate the best fit model among all three. Zhang et al.,2017; Xuefeng et al.,2021 applied machine learning to compare multiple linear regression model and Random Forest algorithm to predict different organic properties of soil and explored that RF perform better than MLR. Huang et al.,2022 conducted a comparative study between  MLR and RF machine learning algorithm in medical field to predict Diabetic Urine Albumin–Creatinine Ratio in a 4-Year Follow-Up and  revealed that RF may advanced  with ability to model highly nonlinear dimensional relationships than other statistical modeling methods. 


The review clearly indicates that a notable number of studies has been conducted on using different statistical tools and methods and their combination to built and compare different predictive models from their respective available dataset in order to determine the best-fit model across all.

A comparative study is important to determine the best-fit model for predicting Sales data. Here MLR and RF algorithm models were tested and compared to check their tendency to predict accurately. 

### Methods

In this study several aspects of data analysis tools were explored to find behavior of each features of the data. However this section comprises of detailed data description, techniques used to predict sales data as well as discusses which model can better explain the prediction. For purpose of this study a MLR was built manually using Matrix method and for exploring RF algorithm in-built package "RandomForest" of R was used. 


#### About the dataset:

Walmart Sales data from (https://www.kaggle.com/datasets/aditya6196/retail-analysis-with-walmart-data?resource=download) has been used for this study. The data is about weekly sales data of 45 Walmart stores along with regional factors like Temperature, Fuel_Price, CPI, Unemployment, holiday flag etc. 


Holiday flag comprises of two factors 1 or 0. If the week contains any of the holidays mentioned below then the flag is set to 1 otherwise 0.

|Holiday Name| Date 1| Date 2| Date 3 |Date 4|
|:-------------:|:--------------------|:------------|:------------:|
|Super Bowl| 12-Feb-10| 11-Feb-11| 10-Feb-12| 8-Feb-13|
|Labor Day |10-Sep-10 |9-Sep-11 |7-Sep-12 |6-Sep-13|
|Thanksgiving |26-Nov-10| 25-Nov-11| 23-Nov-12| 29-Nov-13|
|Christmas |31-Dec-10 |30-Dec-11 |28-Dec-12 |27-Dec-13|



```{r echo=TRUE,warning=FALSE,message=FALSE,include=FALSE}
library(hms)
library(lubridate)
library(tidyverse)
library(reshape2)
library(ggplot2)
library(matlib)
library(dplyr)
library(tidymodels)
library(corrplot)
library(RColorBrewer)
data<-read.csv("Walmart_Store_sales.csv")
#str(data)
# Pre-processing Data
#Converting Holiday_Flag variable to a faxtor variable.
data$Holiday_Flag<-as.factor(data$Holiday_Flag)
#data[,c(1,3:6)]<-as.numeric(data[,c(1,3:6)])
data$Store<-as.factor(data$Store)
#str(data)
dat = data 
#%>% select(-Store,-Date)
dat<- dat %>% mutate_at(c('Weekly_Sales','Temperature','Fuel_Price','CPI','Unemployment'), as.numeric)

#Centralizing each independent continuous variables to avoid problems of Multicollinearity. 
#dat$Temperature<-dat$Temperature-mean(dat$Temperature)
#dat$Fuel_Price<-dat$Fuel_Price-mean(dat$Fuel_Price)
#dat$CPI<-dat$CPI-mean(dat$CPI)
#dat$Unemployment<-dat$Unemployment-mean(dat$Unemployment)
```


A summary of different variables of the data set is shown in the following image.  

```{r echo=TRUE,warning=FALSE,message=FALSE,out.width='25%', fig.align='center', fig.cap=''}
summary(dat)
```
**Fig 1**:Summary of data.


*Checking whether there are NA values.*

```{r echo=TRUE,warning=FALSE,message=FALSE}
library(inspectdf)
y<-inspect_na(dat)
show_plot(y)
```
                                        <li>**Fig 2:** NA or missing values in the data.</li>


Thus no data is missing in the data. 


Exploring types of the data:


```{r}
x<-inspect_types(dat)
show_plot(x)
```
                                        <li>**Fig 3:** Data types.</li> 


For a clean and accurate analysis it is very important to detect and remove outliers from the data. For detecting the outliers boxplot is a powerful tool. First we will check for outliers in the numerical dependent variables.  

```{r echo=FALSE,warning=FALSE,message=FALSE}
dat_long <- melt(dat[,c(4:8)], id = "Holiday_Flag")
ggplot(dat_long, aes(x = variable, y = value,fill="red")) +  # ggplot function
  geom_boxplot()
```
                 <li>**Fig 4:**Boxplots showing outliers in the numerical predictors.</li>


Variable employment has several outliers. So for removing the outliers 25th and 75 quantiles are calculated and the data points lying outside this quantile range are considered as outliers.  

```{r echo=TRUE,warning=FALSE,message=FALSE}

quartiles <- quantile(dat$Unemployment, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dat$Unemployment)
 
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 
 
data_no_outlier <- subset(dat, dat$Unemployment > Lower & dat$Unemployment < Upper)

```


Dependent variable also has a large number of outliers. So it is also crucial to remove them

```{r echo=FALSE,warning=FALSE,message=FALSE}
boxplot(data_no_outlier$Weekly_Sales,col="green")
```
                            <li>**Fig 5:**Boxplots showing outliers in the dependent variable.</li>

Here the dependent variable is upper skewed , thus it is very important to remove the outliers for getting a less error model.

```{r echo=TRUE,warning=FALSE,message=FALSE}

quartiles <- quantile(data_no_outlier$Weekly_Sales, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(data_no_outlier$Weekly_Sales)
 
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 
data_no_outlier<-subset(data_no_outlier,data_no_outlier$Weekly_Sales>Lower &data_no_outlier$Weekly_Sales<Upper)
dat<-as.data.frame(data_no_outlier)
```

Below plot shows distribution of numerical predictors in the data.

```{r echo=FALSE,warning=FALSE,message=FALSE}
num<-inspect_num(dat)
show_plot(num)
```
                        <li>**Fig 6:** Histogram showing data distribution of numerical variables.</li> 


```{r,echo=FALSE,warning=FALSE,message=FALSE}
cat<-inspect_cat(dat[,(3:8)])
show_plot(cat)
```
                              <li>*Fig 7:* Distribution of  Categorical Variable holiday_Flag.</li>

Only few days are holidays. 


```{r echo=FALSE,warning=FALSE,message=FALSE}
cor<-inspect_cor(dat,with_col = "Weekly_Sales")
cor
show_plot(cor)
```
       <li>Fig 8: Plots showing correlation between dependent and numerical independent predictors.</li>


Plot shows that weekly_sales has a significant negative correlation with Unemployment, CPI, Temperature while Fuel-price does not show any significant correlation.  


Sales were averaged over week per year to check how Weekly Sales varies depending on holidays 


```{r echo=FALSE,warning=FALSE,message=FALSE}
df<-cbind.data.frame(dat$Weekly_Sales,dmy(dat$Date))
df <- data.frame(date = df$`dmy(dat$Date)`,df$`dat$Weekly_Sales`,dat$Store,
                 year = as.numeric(format(df$`dmy(dat$Date)`, format ="%Y")),
                 month = as.numeric(format(df$`dmy(dat$Date)`, format = "%m")),
                 day = as.numeric(format(df$`dmy(dat$Date)`, format = "%d")),week=strftime(df$`dmy(dat$Date)`,format = "%V"))
df_dt<-as.data.frame(sapply(df[,c(2:7)],as.integer))
df_dt$year<-as.factor(df_dt$year)
df_dt$dat.Store<-as.factor(df_dt$dat.Store)
names(df_dt)<-c("Weekly_Sales","Store","Year","Month","Day","Week")
df_dt<-aggregate(df_dt$Weekly_Sales,by=list(df_dt$Year,df_dt$Week),FUN=mean) 

names(df_dt)<-c("Year","Week","Weekly_Sales")
df_dt%>%  ggplot(aes(Week,Weekly_Sales,group=Year,color=Year))+geom_line()+ggtitle("Plot showing varition of sales per week")

```
                            <li>Fig 9: Variation of sales with week per year.</li> 


Figure 9 reveals that each year there is a hike in sales in holiday weeks specially between 47-52. Thus holiday flags impact sales positively. 

In the later sections we will discuss about comparing different prediction models.

**Fitting Multiple linear regression model**

The most common Multiple regression model equation is :

$Y=\beta_{0}+\beta_{1} \times X_{1}+\beta_{2} \times X_{2}+........+\beta_{k} \times X_{k}+\varepsilon$

where k=number of regressors or predictors=5 for our model.
      p=number of parameters.
      p=k+1=6
      n=total number of observations=total number of rows in the data=5954.
      $\varepsilon$ = error terms or uncertainty in predictions.
 
This equation can be written in matrix form as:

  $Y=\beta X+\varepsilon$
      
where, $$Y=\begin{bmatrix} y1 \\ 
y2 \\ 
.\\
.\\
.\\
y_{n}\\
\end{bmatrix}$$  Observed outcomes.   

and $$X=\begin{bmatrix} 
1&x_{11}  & x_{12} & x_{13} &.&.&.x_{1k} \\ 
1&x_{21}  & x_{22} & x_{23} &.&.&.x_{2k} \\
.\\
.\\.\\
1&x_{n1}  & x_{n2} & x_{n3} &.&.&.x_{nk}
\end{bmatrix}
$$ predictor matrix.   


random error matrix is 

$$\varepsilon=\begin{bmatrix}
\varepsilon_{1}\\
\varepsilon_{2}\\
.\\
.\\
\varepsilon_{n}\\
\end{bmatrix}
$$ and our coefficient matrix is 

$$\beta=\begin{bmatrix}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
.\\
.\\
\beta_{k}
\end{bmatrix}
$$   Coefficient matrix


Here noticeable part in matrix X is that first column is containing all 1s.The importance of this is that when we multiply our coefficient matrix $\beta$ with predictor matrix X then first term of each equation formed will be $\beta_{0}$. Therefore the coefficient of X we multiply with $\beta_{0}$ is simply 1. So it is crucially important that we create a column of 1s as first column in  model matrix X.  


In order to get a best fit multiple linear regression model we must minimize the sum of squared error.

Here the sum of squared error L is:
 $$L=\sum_{i=1}^{n}{\varepsilon_{i}}^{2}=(y-X\beta)^{'}(y-X\beta)$$

The least square estimator $\hat\beta$ is the solution for $\beta$ in the equation:

$\displaystyle \frac{\partial L}{\partial \beta}=0$. 

Here $\hat\beta$ is a vector that contains all our estimates for the parameters in our model.

The minimization of L in calculus leads to the normalization equation:
 $X^{'}X\hat\beta=X^{'}Y$
 
 Solution of this normal equation leads to solution for $\hat\beta$ as:
 
 $\hat\beta=(X^{'}X)^{-1}X^{'}Y$


```{r echo=FALSE,warning=FALSE,message=FALSE}

df<-dat[,3:8] %>% mutate(D=1)
#creating Y-vector.
Y_train<-matrix(df$Weekly_Sales,ncol = 1)

#creating X-vector.
X_train<-matrix(c(df$D,df$Holiday_Flag,df$Temperature,df$Fuel_Price,df$CPI,df$Unemployment),ncol = 6)
k<-ncol(dat[,c(-1)])
p<-k+1
n<-nrow(Y_train)

#taking transpose of X-vactor.
XT<-t(X_train)
#XT[,1:6]
#head(XT)

#multiply X-transpose by x.
XTX<-XT %*% X_train

#take inverse of multiple.
XTXinv<-inv(XTX)

#calculation XTY
XTY<-XT %*% Y_train

#calculating Beta.
Beta<-XTXinv %*% XTY
```


$\hat\beta$ estimates for multiple linear model are:


|Coefficients|  Estimates|
|:------------:|:-----------|
|$Intercept(\beta_{0})$|1606805.50303|
|$\beta_{1}$|44065.48933|
|$\beta_{2}$|715.73962|
|$\beta_{3}$|-2613.74345|
|$\beta_{4}$|66.54202|
|$\beta_{5}$|-39945.86093|
                             Table 1: Coefficient estimates.

For evaluating significance and performance of the models statistical properties like F-statistics, $R^2$, standard error associated would be calculated.


<li>residuals $e=(y-\hat y)$</li>

Squared Standard error(SSE) $$\hat\sigma_{2}=\frac{\sum_{i=1}^{n}e_{i}^{2}}{n-p}$$ 


Root Mean Square Error(RMSE) $$RMSE=\sqrt\frac{\sum_{i=1}^{n}e_{i}^{2}}{n}$$ and


Mean Absolute Error(MAE)   $$MAE=\frac{\sum_{i=1}^{n}|(Y_{i}-\hat Y_{i})|}{n}=\frac{\sum e_{i}}{n}$$

```{r echo=FALSE,warning=FALSE,message=FALSE}
e<-(Y_train-X_train %*% Beta)
#residuals
#e

SSE<-sum((e^2))^2/(n-p)
RMSE<-sqrt(sum(e^2)/n)
MAE<-(sum(abs(e)))/n
cbind(SSE,RMSE,MAE)

```

Now our regression sum of squares $$ SSR=\sum_{i=1} ^n(\hat y_{i}-\bar y)^{2}$$ and the total sum of squares is
$$SST=\sum_{i=1} ^n (y_{i}-\bar y)^{2}$$ 

```{r echo=FALSE,warning=FALSE,message=FALSE,include=FALSE}
SSR<-sum((X_train %*% Beta-mean(Y_train))^2)
SST<-sum((Y_train-mean(Y_train))^2)
```
 
 Now we will run a F-statistics test for obtaining significance of our overall model.
 
 F-statistics is $\frac{\frac{SSR}{k}}{SSE}$
 
```{r echo=TRUE,warning=FALSE,message=FALSE}
F0<-(SSR)/(SSE)
pf(F0,k,n-p)
```


Here the p-value from f-statistics test indicates that model is statistically significant. Now to evaluate model performance we will calculate $R^{2}$ value by using the formula.  $R^{2}=1-\frac{SSR}{SST}=0.0411$ and standard error=$\sqrt(\frac{SST}{n})=553772$


```{r echo=FALSE,warning=FALSE,message=FALSE,include=FALSE}
R2_mlr<-SSR/(SST*10)
error<-sqrt(SST/n)
data.frame(R2_mlr,error)
```



Now same model parameters will be estimated using random forest regression method. 

*Random forest*

```{r echo=TRUE,message=FALSE,warning=FALSE}
library(randomForest)
rf<-randomForest(Weekly_Sales~.,dat[,3:8],ntree = 500)
print(rf)
```


By applying the above formulas the statictical summery table(RMSE, MAE, $R^2$ and p-value) for Random Forest Model is:


```{r echo=FALSE,message=FALSE,warning=FALSE,include=FALSE}
y_pred = rf$predicted
e_rf<-(Y_train-y_pred)^2
#residuals
#e

SSE_rf<-sum((e_rf)^2)/(n-p)
RMSE<-sqrt(sum((e_rf))/n)
MAE<-(sum(sqrt(e_rf)))/n
cbind(RMSE,MAE)
```


```{r echo=FALSE,message=FALSE,warning=FALSE,include=FALSE}
SSR_rf<-sum((y_pred-(Y_train))^2)
SST_rf<-sum((Y_train-mean(Y_train))^2)
```


```{r echo=FALSE,warning=FALSE,message=FALSE,include=FALSE}
F0_rf<-(SSR_rf/k)/(SSE_rf)
pf(F0_rf,5,n-p)
```

```{r echo=TRUE,warning=FALSE,message=FALSE,include=FALSE}
error_rf<-sqrt(rf$mse[500])
error_rf
```


```{r echo=TRUE,warning=FALSE,message=FALSE,include=FALSE}
R2<-rf$rsq[500]
R2
```

|RMSE|MAE|$R^2$|p-value|
|:----------:|:------------------|:--------------:|:------------:|
|472281.1 |400426.8|0.1355375|2.095769e-22|


### Results and Discussion:

Sales per week for different are influenced by several factors, as found in this study there are sales hike during holidays. Unemployment and temperature affect sales negatively whereas CPI influence is positive. 

The model comparision summarization table is:

|Model Name |RMSE|MAE|$R^2$|p-value|
|:-------------------:|:----------:|:------------------|:--------------:|:------------:|
|MLR|655051| 561616.5|0.04105873	|1.1478e-30|
|RF|472281.1 |400426.8|0.1355375|7.70231e-22|
                               Table 2: Compression table of MLR and RF.

Higher RMSE and MAE for MLR model indicate better performance for RF algorithm. For MLR model a $R^2$ value of 0.041 indicates that the model can only explain 4% variation in the dependent variable which is not reliable to explain our dependent variable. Similarly for RF $R^2$ value of 0.14 indicates that 13% variation of dependent variable can be explained by the model which is of course better than MLR. BOth the models are statistically significant as the p-value is much lower than typical 0.05. 



### Conclusion:

Sales increases during holidays whereas on hot days sales tend to decrease, also if in the area of the store, people unemployment rate is higher sales are likely to go down. Among fitted models LR performed better than MLR with higher $R^2$ and lower RMSE and MAE. 


###	Data and software availability:

Here Walmart retail data (https://www.kaggle.com/datasets/aditya6196/retail-analysis-with-walmart-data?resource=download) was used for the analysis. 

and all the analyses were conducted using R-Studio. Link to download the software is (https://cran.r-project.org/bin/windows/base/R-4.2.2-win.exe).


###	References

<li> 1) Huan Zhang, Pengbao Wu, Aijing Yin, Xiaohui Yang, Ming Zhang, Chao Gao, 2017. "Prediction of soil organic carbon in an intensively managed reclamation zone of eastern China: A comparison of multiple linear regressions and the random forest model, Science of The Total Environment", Volume 592,2017,Pages 704-713,ISSN 0048-9697,https://doi.org/10.1016/j.scitotenv.2017.02.146. </li>


<li> 2) Xuefeng Xie, Tao Wu, Ming Zhu, Guojun Jiang, Yan Xu, Xiaohan Wang, Lijie Pu, 2021. "Comparison of random forest and multiple linear regression models for estimation of soil extracellular enzyme activities in agricultural reclaimed coastal saline land, Ecological Indicators", Volume 120,2021,106925,ISSN 1470-160X,https://doi.org/10.1016/j.ecolind.2020.106925. </li>


<li> 3) Huang, Li-Ying, Fang-Yu Chen, Mao-Jhen Jhou, Chun-Heng Kuo, Chung-Ze Wu, Chieh-Hua Lu, Yen-Lin Chen, Dee Pei, Yu-Fang Cheng, and Chi-Jie Lu. 2022. "Comparing Multiple Linear Regression and Machine Learning in Predicting Diabetic Urine Albumin–Creatinine Ratio in a 4-Year Follow-Up Study" Journal of Clinical Medicine 11, no. 13: 3661. https://doi.org/10.3390/jcm11133661 </li>

<li> 4) Phan Thanh Noi, ORCID,Jan Degener, Martin Kappas. 2017 "Comparison of Multiple Linear Regression, Cubist Regression, and Random Forest Algorithms to Estimate Daily Air Surface Temperature from Dynamic Combinations of MODIS LST Data" Remote Sens. 2017, 9(5), 398; https://doi.org/10.3390/rs9050398 </li>